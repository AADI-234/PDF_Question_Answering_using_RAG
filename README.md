# PDF Chatbot with Langchain, Streamlit, and Google AI

A simple web application that allows users to upload PDF documents and have a conversation about their content. Built using Python, Streamlit, Langchain, FAISS, and Google Generative AI (Gemini).

## Description

This application implements a Retrieval-Augmented Generation (RAG) pipeline:

1.  **Upload:** Users upload a PDF file through a web interface.
2.  **Process:** The application extracts text from the PDF, splits it into manageable chunks, generates embeddings for each chunk using Google's embedding model, and stores these embeddings in a FAISS vector store.
3.  **Chat:** Users can ask questions about the PDF content. The application performs a similarity search on the vector store to find relevant text chunks.
4.  **Generate:** The retrieved chunks, along with the user's question and a sophisticated prompt, are sent to Google's Gemini LLM to generate an answer based *only* on the document's context.
5.  **Display:** The answer and the source text segments used for generation are displayed to the user.

## Features

* Upload PDF documents via a Streamlit interface.
* Ask questions about the content of the uploaded PDF.
* Receive answers generated by Google's Gemini model based *only* on the document context.
* View the specific text segments retrieved from the document that were used to generate the answer.
* PDF processing occurs only once per session for efficiency using `st.session_state`.
* Caching is implemented for loading vector stores and initializing the QA chain (`@st.cache_resource`).

## Tech Stack

* **Language:** Python 3.9+
* **Web Framework:** Streamlit
* **LLM Framework:** Langchain
* **LLM & Embeddings:** Google Generative AI (Gemini 1.5 Pro, `embedding-001`)
* **Vector Store:** FAISS (Facebook AI Similarity Search) - CPU version
* **PDF Parsing:** PyPDF2
* **Environment Variables:** python-dotenv

## Project Structure

```
.
├── vector_stores/      # Directory to store generated FAISS indexes
├── .env                # Stores API keys (Important: Add to .gitignore!)
├── app.py              # Main Streamlit application file (UI, workflow)
├── document_processor.py # Handles PDF parsing, chunking, embedding, vector store creation
├── requirements.txt    # Project dependencies
└── README.md           # This file
```

## Setup and Installation

Follow these steps to set up and run the project locally:

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/AADI-234/PDF_Question_Answering_using_RAG
    cd PDF_Question_Answering_using_RAG 
    ```

2.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv myvenv 
    # On Windows
    myvenv\Scripts\activate 
    # On macOS/Linux
    source myvenv/bin/activate 
    ```
    *(Note: Use the correct activation command for your environment name `myvenv`)*

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set Up Environment Variables:**
    * Create a file named `.env` in the project's root directory (where `app.py` is located).
    * Add your Google Generative AI API key to this `.env` file:
        ```env
        GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY_HERE"
        ```
    * **Important:** Make sure the `.env` file is listed in your `.gitignore` file if you are using Git, to prevent accidentally committing your secret API key. The application uses the `python-dotenv` library to automatically load this key from the `.env` file when it starts.

## Usage

1.  Ensure your virtual environment (`myvenv`) is activated.
2.  Run the Streamlit application from your terminal:
    ```bash
    streamlit run app.py
    ```
3.  Open your web browser and navigate to the local URL provided by Streamlit (usually `http://localhost:8501`).
4.  Upload a PDF file using the file uploader.
5.  Wait for the processing to complete (a success message will appear).
6.  Ask questions about the PDF content in the text input box.

---
